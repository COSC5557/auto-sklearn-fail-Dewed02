{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b37d9f-ef7f-4f0d-b978-85bb1e85d5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy 0.67\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection\n",
    "from sklearn.datasets import fetch_openml\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "X, y = fetch_openml(data_id=40691, as_frame=True, return_X_y=True)\n",
    "\n",
    "columns = X.select_dtypes(include=['category', 'int']).columns # Determine if any columns in feature data are categorical/integer \n",
    "\n",
    "if not columns.empty:\n",
    "    enc = OneHotEncoder() # One hot encoding should only be done for categorical/interger values\n",
    "    X = enc.fit_transform(X)  \n",
    "    \n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "print(\"RF Accuracy\", sklearn.metrics.accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f41e7901-105f-40b0-b9eb-291d12fd8532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalton5/.local/lib/python3.9/site-packages/autosklearn/data/target_validator.py:187: UserWarning: Fitting transformer with a pandas series which has the dtype category. Inverse transform may not be able preserve dtype when converting to np.ndarray\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML Accuracy 0.6725\n"
     ]
    }
   ],
   "source": [
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "\n",
    "automl = AutoSklearnClassifier(time_left_for_this_task=300,\n",
    "                              resampling_strategy='cv',\n",
    "                              seed=349)\n",
    "automl.fit(X_train, y_train)\n",
    "y_hat = automl.predict(X_test)\n",
    "print(\"AutoML Accuracy\", sklearn.metrics.accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a3809-fa55-4d77-bbf1-f59dc303d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "There were two significant issues that I noticed with this example. First, the use of one hot encoding appears to have been done without regard to whether the dataset would benefit from it. This is especially true as the \n",
    "dataset used in this example is one of the wine quality datasets; the feature data consists entirely of continuous data. Encoding values in the feature space would likely be harmful because all values would be converted\n",
    "to ones and zeros, which would likely result in the loss of meaning of a lot of values.  This preprocessing approach is more applicable to categorical or integer values where the values of one element do not indicate a \n",
    "greater or lesser quality/quantity in regard to other elements. \n",
    "\n",
    "The second issue is that the hyperparameter search space is not specified for the auto-sklearn model to tune over. This may not be an issue, as the auto-sklearn documentation states that a large search space is used by\n",
    "default. However, it is better and more computationally efficient to use ranges in your search space that are known to produce good results. By simply specifying the resampling method, the model if only marginally shows\n",
    "better performance than its default counter part. \n",
    "\n",
    "While these both contribute to poor performance, from my experience, the factor that caused the auto-sklearn model to perform worse than its default counterpart is the lack of defintion of the hyperparameter search\n",
    "space. One hot encoding did drag down the performance of both moodels quite significantly, but the auto-sklearn model only showed better performance when the resampling strategy was specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c32039-b392-4d39-9f23-ed7b004024ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sources: \n",
    "    Lekhana_Ganji. (2023, April 18). One hot encoding in machine learning. GeeksforGeeks. https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/# \n",
    "    @inproceedings{feurer-neurips15a,\n",
    "    title     = {Efficient and Robust Automated Machine Learning},\n",
    "    author    = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},\n",
    "    booktitle = {Advances in Neural Information Processing Systems 28 (2015)},\n",
    "    pages     = {2962--2970},\n",
    "    year      = {2015}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1703a-a708-430c-adb5-58157735091a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
